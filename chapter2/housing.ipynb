{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function fetch_housing that\n",
    "# downloads a csv-file with testdata from a url into a local file \n",
    "\n",
    "import os         # create and read local files\n",
    "import tarfile    # extract from tarfiles\n",
    "import requests   # download from url\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL =  DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    \n",
    "    socket = requests.get(housing_url, stream=True)\n",
    "    with open(tgz_path, 'wb') as fd:\n",
    "        for chunk in socket.iter_content(chunk_size=128):\n",
    "            fd.write(chunk)\n",
    "        \n",
    "    housing_tgz =tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)    # this extracts housing.csv\n",
    "    housing_tgz.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function load_housing\n",
    "# that constructs a panda data-object from a local csv-file\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create the data-object housing and list the first 5 lines\n",
    "housing = load_housing_data()\n",
    "housing.head()   # shows the first lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()   # shows number/types of lines/columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"ocean_proximity\"].value_counts()  # categorizes and counts values for one col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe()      # simple statistics on numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not part of the handson book:\n",
    "# a first example using matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data for plotting\n",
    "t = np.arange(0.0, 2.0, 0.01)\n",
    "s = 1 + np.sin(2 * np.pi * t)\n",
    "# Note that using plt.subplots below is equivalent to using\n",
    "# fig = plt.figure() and then ax = fig.add_subplot(111)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(t, s)\n",
    "ax.set(xlabel='time (s)', ylabel='voltage (mV)',\n",
    "       title='About as simple as it gets, folks')\n",
    "# ax.grid()\n",
    "# fig.savefig(\"test.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show histograms of all numeric data in housing\n",
    "housing.hist(bins=50, figsize=(20,15))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset in training- and testset\n",
    "# the following method would randomly separate a ration of data points\n",
    "# where we use a constant seed to make sure we make the same choice next time\n",
    "import numpy as np\n",
    "\n",
    "def split_random(data, testRatio):\n",
    "    np.random.seed(42)\n",
    "    randomIndices = np.random.permutation(len(data))\n",
    "    testSize = int(len(data) * testRatio)\n",
    "    testIndices = randomIndices[:testSize]\n",
    "    trainIndices = randomIndices[testSize:]\n",
    "    return data.iloc[trainIndices], data.iloc[testIndices]\n",
    "\n",
    "trainSet, testSet = split_random(housing, 0.2)\n",
    "print(len(trainSet), \"train and \", len(testSet), \" test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to have a split that is also stable when part of the data is \n",
    "# deleted / completed / added it is a nice idea to base the split\n",
    "# on the hash of an ID, that here we first construct from a stable attribute\n",
    "\n",
    "import hashlib\n",
    "\n",
    "housing_withID = housing\n",
    "housing_withID[\"id\"] = housing[\"longitude\"]*1000 + housing[\"latitude\"]\n",
    "\n",
    "# a function that returns true for a given proportion of ids\n",
    "# this is called by the lambda expression below to filter an array of ids\n",
    "def isInTest(id, testRatio, hash):\n",
    "    hashedID = hash(np.int64(id))\n",
    "    return hashedID.digest()[-1] < 256 * testRatio   # compare the last byte of the hash ?\n",
    "\n",
    "def split_byID(data, testRatio, idColumn, hash=hashlib.md5):\n",
    "    ids = data[idColumn]\n",
    "    testIDs = ids.apply(lambda id_ : isInTest(id_, testRatio, hash))\n",
    "    # print(testIDs) # this is a Dataframe column of booleans, i.e. not\n",
    "    # just a 1D array of booleans or indices, but something like:\n",
    "    #  0   false\n",
    "    #  1   false\n",
    "    #  2   true\n",
    "    #  ...\n",
    "    return data.loc[-testIDs], data.loc[testIDs]\n",
    "    \n",
    "trainSet, testSet = split_byID(housing, 0.3, \"id\")\n",
    "print(len(trainSet), \"train and \", len(testSet), \" test\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a predefined function in sklearn does the same as our first, i.e. it \n",
    "# is not stable against data changes. It has the advantage that it can be applied\n",
    "# to >1 table to select from each the same rows (not shown here)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "trainSet, testSet = train_test_split(housing, test_size=0.25, random_state=42)\n",
    "print(len(trainSet), \"train and \", len(testSet), \" test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to have a test set that is (guaranteed) representative with respect to the\n",
    "# income attribute, we first add the income as a category (i.e. either 0,1,2,3,4,5) \n",
    "# calculated from the (relative) income that is a number (between 0 and > 15)\n",
    "# by rounding to integers and capping at a value of 5\n",
    "housing[\"income_cat\"] = np.ceil(housing[\"median_income\"]/ 1.5 )\n",
    "housing[\"income_cat\"].where(housing[\"income_cat\"] < 5, 5.0, inplace=True)\n",
    "housing[\"income_cat\"].hist()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "StratifiedShuffleSplit = sklearn.model_selection.StratifiedShuffleSplit\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "mySplit = splitter.split(housing, housing[\"income_cat\"])\n",
    "# mySplit is a \"generator Object\" which is a common concept in python\n",
    "# a collection that can only be inspected with \"for in\", i.e. an Iterable\n",
    "# the for-loop is executed only once, seems to be the equivalence of:\n",
    "# train_index = mySplit.next()\n",
    "# test_index = mySplit.next()\n",
    "for train_index, test_index in mySplit:\n",
    "    # the produces index-sets seem to be arrays of indices ?\n",
    "    # because we have no separate row-labels we can use loc and iloc\n",
    "    strat_train_set = housing.loc[train_index]   \n",
    "    strat_test_set = housing.iloc[test_index]\n",
    "    print(\"len train index:\" , len(train_index))\n",
    "\n",
    "# to prove the representative split:\n",
    "# watchout: train and test are dataframes with rows sorted wrt\n",
    "# frequency, must use loc (and not iloc) to get our categories\n",
    "# in the order that we expect\n",
    "train = strat_train_set[\"income_cat\"].value_counts() / len(strat_train_set)\n",
    "test = strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)\n",
    "for i in [1.0, 2.0, 3.0, 4.0, 5.0]:\n",
    "    print(\"%5d %10.4f %10.4f\" % (i, train.loc[i], test.loc[i]))\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
