{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function fetch_housing that\n",
    "# downloads a csv-file with testdata from a url into a local file \n",
    "\n",
    "import os         # create and read local files\n",
    "import tarfile    # extract from tarfiles\n",
    "import requests   # download from url\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL =  DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    \n",
    "    # create directory where we will save a tar-file\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    \n",
    "    # perform a get on the url and save the content to our local tar-file\n",
    "    socket = requests.get(housing_url, stream=True)\n",
    "    with open(tgz_path, 'wb') as fd:\n",
    "        for chunk in socket.iter_content(chunk_size=128):\n",
    "            fd.write(chunk)\n",
    "            \n",
    "    # open the local tar-file and extract its content in the same directory\n",
    "    housing_tgz =tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)    # this extracts housing.csv\n",
    "    housing_tgz.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function load_housing\n",
    "# that constructs a panda data-object from a local csv-file\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "    \n",
    "# numpy and matplotlib will be required in many cells\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create the data-object housing and list the first 5 lines\n",
    "housing = load_housing_data()\n",
    "housing.head()   # shows the first lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()   # shows number/types of lines/columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"ocean_proximity\"].value_counts()  # categorizes and counts values for one col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe()      # simple statistics on numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show histograms of all numeric data in housing\n",
    "housing.hist(bins=50, figsize=(20,15))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset in training- and testset\n",
    "# the following method would randomly separate a ration of data points\n",
    "# where we use a constant seed to make sure we make the same choice next time\n",
    "import numpy as np\n",
    "\n",
    "def split_random(data, testRatio):\n",
    "    np.random.seed(42)\n",
    "    randomIndices = np.random.permutation(len(data))\n",
    "    testSize = int(len(data) * testRatio)\n",
    "    testIndices = randomIndices[:testSize]\n",
    "    trainIndices = randomIndices[testSize:]\n",
    "    return data.iloc[trainIndices], data.iloc[testIndices]\n",
    "\n",
    "trainSet, testSet = split_random(housing, 0.2)\n",
    "print(len(trainSet), \"train and \", len(testSet), \" test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to have a split that is also stable when part of the data is \n",
    "# deleted / completed / added it is a nice idea to base the split\n",
    "# on the hash of an ID, that here we first construct from a stable attribute\n",
    "\n",
    "import hashlib\n",
    "\n",
    "housing_withID = housing\n",
    "housing_withID[\"id\"] = housing[\"longitude\"]*1000 + housing[\"latitude\"]\n",
    "\n",
    "# a function that returns true for a given proportion of ids\n",
    "# this is called by the lambda expression below to filter an array of ids\n",
    "def isInTest(id, testRatio, hash):\n",
    "    hashedID = hash(np.int64(id))\n",
    "    return hashedID.digest()[-1] < 256 * testRatio   # compare the last byte of the hash ?\n",
    "\n",
    "def split_byID(data, testRatio, idColumn, hash=hashlib.md5):\n",
    "    ids = data[idColumn]\n",
    "    testIDs = ids.apply(lambda id_ : isInTest(id_, testRatio, hash))\n",
    "    # print(testIDs) # this is a Dataframe column of booleans, i.e. not\n",
    "    # just a 1D array of booleans or indices, but something like:\n",
    "    #  0   false\n",
    "    #  1   false\n",
    "    #  2   true\n",
    "    #  ...\n",
    "    return data.loc[-testIDs], data.loc[testIDs]\n",
    "    \n",
    "trainSet, testSet = split_byID(housing, 0.3, \"id\")\n",
    "print(len(trainSet), \"train and \", len(testSet), \" test\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a predefined function in sklearn does the same as our first, i.e. it \n",
    "# is not stable against data changes. It has the advantage that it can be applied\n",
    "# to >1 table to select from each the same rows (not shown here)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "trainSet, testSet = train_test_split(housing, test_size=0.25, random_state=42)\n",
    "print(len(trainSet), \"train and \", len(testSet), \" test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# in order to have a test set that is (guaranteed) representative with respect to the\n",
    "# income attribute, we first add the income as a category (i.e. either 0,1,2,3,4,5) \n",
    "# calculated from the (relative) income that is a number (between 0 and > 15)\n",
    "# by rounding to integers and capping at a value of 5\n",
    "housing[\"income_cat\"] = np.ceil(housing[\"median_income\"]/ 1.5 )\n",
    "housing[\"income_cat\"].where(housing[\"income_cat\"] < 5, 5.0, inplace=True)\n",
    "housing[\"income_cat\"].hist()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "StratifiedShuffleSplit = sklearn.model_selection.StratifiedShuffleSplit\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "mySplit = splitter.split(housing, housing[\"income_cat\"])\n",
    "# mySplit is a \"generator Object\" which is a common concept in python\n",
    "# a collection that can only be inspected with \"for in\", i.e. an Iterable\n",
    "# the for-loop is executed only once, seems to be the equivalence of:\n",
    "# train_index = mySplit.next()\n",
    "# test_index = mySplit.next()\n",
    "for train_index, test_index in mySplit:\n",
    "    # the produces index-sets seem to be arrays of indices ?\n",
    "    # because we have no separate row-labels we can use loc and iloc\n",
    "    strat_train_set = housing.loc[train_index]   \n",
    "    strat_test_set = housing.iloc[test_index]\n",
    "    print(\"len train index:\" , len(train_index))\n",
    "\n",
    "# to prove the representative split:\n",
    "# watchout: train and test are dataframes with rows sorted wrt\n",
    "# frequency, must use loc (and not iloc) to get our categories\n",
    "# in the order that we expect\n",
    "train = strat_train_set[\"income_cat\"].value_counts() / len(strat_train_set)\n",
    "test = strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)\n",
    "for i in [1.0, 2.0, 3.0, 4.0, 5.0]:\n",
    "    print(\"%5d %10.4f %10.4f\" % (i, train.loc[i], test.loc[i]))\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize data, first the geographical data\n",
    "housing = strat_train_set\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1\n",
    "             , s=np.log(housing[\"population\"]), c=\"median_house_value\"\n",
    "             , cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
    "            label=\"population\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the correlation matrix for all attributes, and sort for correlation with house_value\n",
    "correlation = housing.corr()\n",
    "correlation[\"median_house_value\"].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce the matrix of scatterplots, here only for three attributes\n",
    "from pandas.plotting import scatter_matrix\n",
    "attributes=[\"median_house_value\", \"median_income\", \"total_rooms\"]\n",
    "scatter_matrix(housing[attributes], figsize=(12,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that in the most important scatter-plot (income vs value) we have quirks in the data\n",
    "# i.e. horizontal lines that must have artificial origin\n",
    "housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\", alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since neither the total number of rooms / bedrooms (per district) nor the total\n",
    "# population seem meaningfull, we combine with number of households and recalculate\n",
    "# correlation to see that (obviously) rooms-per-house correlates and (surprisingly)\n",
    "# bedrooms-per-room had a strong (negative) correlation\n",
    "housing[\"rooms_per_house\"] = housing[\"total_rooms\"] / housing[\"households\"]\n",
    "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "housing[\"population_per_house\"]= housing[\"population\"] / housing[\"households\"]\n",
    "housing.corr()[\"median_house_value\"].sort_values()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data, we start with our stratified split, and now we also separate\n",
    "# the predictors (\"x-data\") from the labels \"y-value\"\n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()\n",
    "\n",
    "# Most columns are complete, but for some districts the column total_bedrooms\n",
    "# is missing. We could crop the complete column (with drop) or use \"dropna\" to remove districts\n",
    "# that dont have this value or use \"fillna\" to complete those missing values with the median of others:\n",
    "# (the correct approach is to later use the exact same median of training data for missing test data)\n",
    "housing_noNA = housing.dropna(subset=[\"total_bedrooms\"])   # doesn't change housing itself\n",
    "housing.fillna(housing[\"total_bedrooms\"].median(), inplace=True) # changes housing due to inplace=true\n",
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same processing is implemented in sklearn by the Imputer class. We must remove the ocean-proximity \n",
    "# because it is non-numeric and would lead to an error, also Imputer produces a numpy-array which we \n",
    "# must transform back in a dataframe. The important point: the Imputer is a Transformer, and data-preparation\n",
    "# is typically performed in a pipeline of different transformers, each of them is instantiated, then\n",
    "# prepared with a transformer.fit() e.g. to calculate here the median and applied with transformer.transform()\n",
    "\n",
    "from sklearn.impute import SimpleImputer \n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "imputer = sklearn.impute.SimpleImputer(strategy=\"median\")\n",
    "imputer.fit(housing_num)\n",
    "(imputer.statistics_ - housing_num.median().values).sum()  # check: imputer.statistics_ has median values\n",
    "housing_fillna = pd.DataFrame(imputer.transform(housing_num), columns=housing_num.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add ocean proximity \n",
    "# with factorize we could transform text to numeric categories\n",
    "prox_col, prox_cats = housing[\"ocean_proximity\"].factorize()\n",
    "prox_col     # array([0, 0, 1, 2, 0, 2, 0, 2, 0, 0, .....])\n",
    "prox_cats    # Index(['<1H OCEAN', 'NEAR OCEAN', 'INLAND', 'NEAR BAY', 'ISLAND'], dtype='object')\n",
    "\n",
    "\n",
    "# but ML-routines would assume category 3 and 4 to be more similar\n",
    "# than 3 and 5, which is not the case. For unrelated categories,\n",
    "# a one-hot encoding is better: each value is replaced by an \n",
    "# scipy.sparsearray of booleans with only one true-value. \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(categories='auto')         # categories=auto: if all values are either 3 or 17\n",
    "                                                   # the one-hot encoder has length 2, not 17\n",
    "prox_col = prox_col.reshape(-1,1)                  # reshapes to 2D array with one column\n",
    "prox_col_oneHot = encoder.fit_transform(prox_col)\n",
    "\n",
    "# the old method required to transform text to ints, then reshape\n",
    "# now conversion to ints no longer needed, the reshape (which requires numpy.array) still is:\n",
    "encoder = OneHotEncoder()   # \n",
    "prox_col_oneHot = encoder.fit_transform(np.array(housing[\"ocean_proximity\"]).reshape(-1,1))\n",
    "\n",
    "# toarray() produces a conventional dense array from a scipy sparse array \n",
    "# here for illustration purpose only:\n",
    "print(prox_col_oneHot.toarray())\n",
    "encoder.categories_\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEITER ab Custom Transformers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
