{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# return_X_y=true gibt nur das Tupel von np.arrays\n",
    "# mnist = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "\n",
    "# default ist ein object mit 'data', 'target', ... attributes\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = mnist[\"data\"], mnist[\"target\"]\n",
    "print(X.shape)\n",
    "sampleDigit = X[36000]\n",
    "plt.imshow(sampleDigit.reshape(28,28), cmap = matplotlib.cm.binary, interpolation = \"nearest\")\n",
    "# plt.axis(\"off\")\n",
    "plt.show\n",
    "y[36000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meine Spielerei: 10x10 images zusammenstellen\n",
    "\n",
    "randomIndices = np.random.permutation(100)\n",
    "digitTable = X[randomIndices]\n",
    "digitTable.shape\n",
    "\n",
    "def digit(i) :\n",
    "    myDigit = X[randomIndices[i]]\n",
    "    return myDigit.reshape(28,28)\n",
    "\n",
    "# print(digit(27))\n",
    "\n",
    "for i in range(0, 9) :\n",
    "    myRow = digit(10*i)\n",
    "    for j in range(1, 9) : \n",
    "        myRow = np.hstack((myRow, digit(10*i+j)))\n",
    "    if i == 0:\n",
    "        digitTable = myRow\n",
    "    else :\n",
    "        digitTable = np.vstack((digitTable, myRow))\n",
    "            \n",
    "\n",
    "plt.imshow(digitTable, cmap = matplotlib.cm.binary, interpolation = \"nearest\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist is already split in 60,000 training and 10,000 test data\n",
    "# we shuffle the training data to have a valid basis for crossvalidation\n",
    "shuffle = np.random.permutation(60000)\n",
    "x_train, x_test, y_train, y_test = X[:60000][shuffle], X[60000:], y[:60000][shuffle], y[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a binary classifier (yes/no) on predicting it's a five or not\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd = SGDClassifier(random_state=42, tol=1e-3, max_iter=1000)\n",
    "sgd.fit(x_train, y_train == '5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement cross-validation ourselves\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "y_train_5 = (y_train == '5')\n",
    "skfolds = StratifiedKFold(n_splits=3, random_state=42)\n",
    "for train_index, test_index in skfolds.split(x_train, y_train_5):\n",
    "    fitter = clone(sgd)\n",
    "    fitter.fit(x_train[train_index], y_train_5[train_index])\n",
    "    y_pred = fitter.predict(x_train[test_index])\n",
    "    print(sum(y_pred == y_train_5[test_index]) / len(y_pred))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following is identical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy not the right measure because only 10% of samples is a '5'\n",
    "# e.g. the trivial predictor always outputting 'false' has accuracy 90%\n",
    "# confuction matrix gives TP, FP, FN, TN and is calculated \n",
    "\n",
    "#  # this would give the exact same result as our implementation of cross-validation\n",
    "#  from sklearn.model_selection import cross_val_score\n",
    "#  cross_val_score(sgd, x_train, (y_train == '5'), cv=3, scoring=\"accuracy\")\n",
    "\n",
    "# use cross validation to calculate a predcition instead of a score, \n",
    "# from which we calculate a confusion matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_train_5 = (y_train == '5')\n",
    "y_train_pred = cross_val_predict(sgd, x_train, (y_train_5), cv=3)\n",
    "m = confusion_matrix(y_train_5, y_train_pred)\n",
    "\n",
    "\n",
    "# confusion_matrix(test, predicted) gives:\n",
    "# \n",
    "#  TN  FP    --  first row: all negativ cases\n",
    "#  FN  TP\n",
    "#       |\n",
    "#      second column: predicted to be positive\n",
    "#\n",
    "print(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m)\n",
    "TN = m[0,0]\n",
    "FP = m[0,1]\n",
    "FN = m[1,0]\n",
    "TP = m[1,1]\n",
    "print(\"accuracy = (TP + TN) / (TP + FP + TN + FN) = \", (TP+TN)/y_train.size)\n",
    "\n",
    "print(\"precision = TP / (TP + FP) = \", TP/(TP+FP))\n",
    "print(\"TPR = recall = TP / (TP + FN)\", TP / (TP+FN))\n",
    "print(\"F1 = harm. mean(precision,TPR) = 2*TP/(2TP+FN+FP)\", 2*TP/(2*TP + FN+ FP))\n",
    "\n",
    "# same thing from sklearn.metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "print(\"precision: \", precision_score(y_train_5, y_train_pred))\n",
    "print(\"recall: \", recall_score(y_train_5, y_train_pred))\n",
    "print(\"f1: \", f1_score(y_train_5, y_train_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the yes/no prediction of SGD is based on the score being higher than\n",
    "# the threshold, which is set at zero. We can improve precision at the cost of\n",
    "# recall by setting a higher treshold\n",
    "# sklearn has no implementation to calculat confusion-matrix for \n",
    "# for different threshold, so we implement ourselves, using decision_function, \n",
    "# to make the decision ourselves\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "def getConfusion(threshold) :\n",
    "\n",
    "    TP = TN = FP = FN = 0\n",
    "    y_train_5 = (y_train == '5')\n",
    "    skfolds = StratifiedKFold(n_splits=3, random_state=42)\n",
    "    for train_index, test_index in skfolds.split(x_train, y_train_5):\n",
    "        fitter = clone(sgd)\n",
    "        fitter.fit(x_train[train_index], y_train_5[train_index])\n",
    "        y_score = fitter.decision_function(x_train[test_index])\n",
    "        y_pred = (y_score > threshold)\n",
    "        TP = TP + sum(y_pred & (y_pred == y_train_5[test_index]))\n",
    "        FP = FP + sum(y_pred & (y_pred != y_train_5[test_index]))\n",
    "        FN = FN + sum(y_train_5[test_index] & (y_pred != y_train_5[test_index]))\n",
    "\n",
    "    TN = TN + (y_train_5.size - TP - FP - FN)\n",
    "    print(\"\\n threshold:\", threshold, \"precision:\", TP / (TP+FP))\n",
    "    print(\"recall:\", TP / (TP+FN))\n",
    "    return np.array([[TN, FP], [FN, TP]])\n",
    "\n",
    "m = getConfusion(0)\n",
    "print(m)\n",
    "print(getConfusion(1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing this threshold is a trade-off between precision and recall\n",
    "# which is illustrated by precision_recall_curve and is implemented in sklearn:\n",
    "\n",
    "# predict the full set of scores\n",
    "y_train_5 = (y_train =='5')\n",
    "y_scores = cross_val_predict(sgd, x_train, y_train_5, cv=3, method=\"decision_function\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the curve\n",
    "# recall is smooth and monotonously decreasing,\n",
    "# precision increases with threshold but not necessarily\n",
    "# monotonously and therefore less smooth\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\n",
    "plt.plot(thresholds, precisions[:-1], \"b--\", label=\"precision\")\n",
    "plt.plot(thresholds, recalls[:-1], \"g-\", label=\"recall\")\n",
    "plt.xlabel(\"threshold\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the (other) classical curve is ROC, plotting \n",
    "# TPR=recall versus FPR = 1 - sensitivity\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\n",
    "plt.plot(fpr, tpr, linewidth=2)\n",
    "plt.plot([0,1], [0,1], 'k--')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.show()\n",
    "\n",
    "# the area under the curve is higher for a better predictor\n",
    "# AUC=0.5 for a random predictor, AUC=1.0 for a perfect predictor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(\"auc:\", roc_auc_score(y_train_5, y_scores))\n",
    "\n",
    "## TIP\n",
    "# use precision vs recall if FP are more important than FN\n",
    "# otherwise use ROC curve / AUC rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now a second classifier: a random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(random_state=42, n_estimators=10)\n",
    "y_predict = cross_val_predict(rfc, x_train, y_train_5,\n",
    "                               cv=3, method=\"predict_proba\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the above y_predict calculated as predict_proba are not\n",
    "# a 60000x1 array either of true/false values or scores,\n",
    "# but rather 60000x2 array with probabilities of\n",
    "# being 5 or not being 5. Select the 2nd column as a \"score\"\n",
    "# to produce ROCplt.plot(fpr, tpr, linewidth=2)\n",
    "y_score = y_predict[:,1]\n",
    "fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_train_5, y_score)\n",
    "\n",
    "plt.plot([0,1], [0,1], 'k--')\n",
    "plt.plot(fpr_rf, tpr_rf, \"b:\")\n",
    "plt.plot(fpr,tpr, \"b-\")\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.show()\n",
    "\n",
    "print(\"auc random forest:\", roc_auc_score(y_train_5, y_score))\n",
    "\n",
    "# at page95, TODO: multiclass classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
