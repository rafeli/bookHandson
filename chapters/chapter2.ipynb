{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function fetch_housing that\n",
    "# downloads a csv-file with testdata from a url into a local file \n",
    "\n",
    "import os         # create and read local files\n",
    "import tarfile    # extract from tarfiles\n",
    "import requests   # download from url\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL =  DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    \n",
    "    # create directory where we will save a tar-file\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    \n",
    "    # perform a get on the url and save the content to our local tar-file\n",
    "    socket = requests.get(housing_url, stream=True)\n",
    "    with open(tgz_path, 'wb') as fd:\n",
    "        for chunk in socket.iter_content(chunk_size=128):\n",
    "            fd.write(chunk)\n",
    "            \n",
    "    # open the local tar-file and extract its content in the same directory\n",
    "    housing_tgz =tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)    # this extracts housing.csv\n",
    "    housing_tgz.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function load_housing\n",
    "# that constructs a panda data-object from a local csv-file\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "    \n",
    "# numpy and matplotlib will be required in many cells\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create the data-object housing and list the first 5 lines\n",
    "housing = load_housing_data()\n",
    "housing.head()   # shows the first lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"ocean_proximity\"].value_counts()  # categorizes and counts values for one col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe()      # simple statistics on numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()   # shows number/types of lines/columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show histograms of all numeric data in housing\n",
    "housing.hist(bins=50, figsize=(20,15))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset in training- and testset\n",
    "# the following method would randomly separate a ration of data points\n",
    "# where we use a constant seed to make sure we make the same choice next time\n",
    "import numpy as np\n",
    "\n",
    "def split_random(data, testRatio):\n",
    "    np.random.seed(42)\n",
    "    randomIndices = np.random.permutation(len(data))\n",
    "    testSize = int(len(data) * testRatio)\n",
    "    testIndices = randomIndices[:testSize]\n",
    "    trainIndices = randomIndices[testSize:]\n",
    "    return data.iloc[trainIndices], data.iloc[testIndices]\n",
    "\n",
    "trainSet, testSet = split_random(housing, 0.2)\n",
    "print(len(trainSet), \"train and \", len(testSet), \" test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to have a split that is also stable when part of the data is \n",
    "# deleted / completed / added it is a nice idea to base the split\n",
    "# on the hash of an ID, that here we first construct from a stable attribute\n",
    "\n",
    "import hashlib\n",
    "\n",
    "housing_withID = housing\n",
    "housing_withID[\"id\"] = housing[\"longitude\"]*1000 + housing[\"latitude\"]\n",
    "\n",
    "# a function that returns true for a given proportion of ids\n",
    "# this is called by the lambda expression below to filter an array of ids\n",
    "def isInTest(id, testRatio, hash):\n",
    "    hashedID = hash(np.int64(id))\n",
    "    return hashedID.digest()[-1] < 256 * testRatio   # compare the last byte of the hash ?\n",
    "\n",
    "def split_byID(data, testRatio, idColumn, hash=hashlib.md5):\n",
    "    ids = data[idColumn]\n",
    "    testIDs = ids.apply(lambda id_ : isInTest(id_, testRatio, hash))\n",
    "    # print(testIDs) # this is a Dataframe column of booleans, i.e. not\n",
    "    # just a 1D array of booleans or indices, but something like:\n",
    "    #  0   false\n",
    "    #  1   false\n",
    "    #  2   true\n",
    "    #  ...\n",
    "    return data.loc[-testIDs], data.loc[testIDs]\n",
    "    \n",
    "trainSet, testSet = split_byID(housing, 0.3, \"id\")\n",
    "print(len(trainSet), \"train and \", len(testSet), \" test\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a predefined function in sklearn does the same as our first, i.e. it \n",
    "# is not stable against data changes. It has the advantage that it can be applied\n",
    "# to >1 table to select from each the same rows (not shown here)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "trainSet, testSet = train_test_split(housing, test_size=0.25, random_state=42)\n",
    "print(len(trainSet), \"train and \", len(testSet), \" test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# in order to have a test set that is (guaranteed) representative with respect to the\n",
    "# income attribute, we first add the income as a category (i.e. either 0,1,2,3,4,5) \n",
    "# calculated from the (relative) income that is a number (between 0 and > 15)\n",
    "# by rounding to integers and capping at a value of 5\n",
    "housing[\"income_cat\"] = np.ceil(housing[\"median_income\"]/ 1.5 )\n",
    "housing[\"income_cat\"].where(housing[\"income_cat\"] < 5, 5.0, inplace=True)\n",
    "housing[\"income_cat\"].hist()\n",
    "plt.show()\n",
    "\n",
    "# and now we use the (new) category to produce a \"stratified split\"\n",
    "# i.e. a split that conserves income-distributions in both sets\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "StratifiedShuffleSplit = sklearn.model_selection.StratifiedShuffleSplit\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "mySplit = splitter.split(housing, housing[\"income_cat\"])\n",
    "\n",
    "# mySplit is a \"generator Object\" which seems a common concept in python\n",
    "# although I definitely dont see its' value here: it seems to be a \n",
    "# a collection that can only be inspected with \"for in\", i.e. an Iterable\n",
    "# the for-loop is executed only once, seems to be the equivalence of:\n",
    "# train_index = mySplit.next()\n",
    "# test_index = mySplit.next()\n",
    "for train_index, test_index in mySplit:\n",
    "    # the produces index-sets seem to be arrays of indices ?\n",
    "    # because we have no separate row-labels we can use loc and iloc\n",
    "    strat_train_set = housing.loc[train_index]   \n",
    "    strat_test_set = housing.iloc[test_index]\n",
    "    print(\"len train index:\" , len(train_index))\n",
    "\n",
    "# to prove the representative split:\n",
    "# watchout: train and test are dataframes with rows sorted wrt\n",
    "# frequency, must use loc (and not iloc) to get our categories\n",
    "# in the order that we expect\n",
    "train = strat_train_set[\"income_cat\"].value_counts() / len(strat_train_set)\n",
    "test = strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)\n",
    "for i in [1.0, 2.0, 3.0, 4.0, 5.0]:\n",
    "    print(\"%5d %10.4f %10.4f\" % (i, train.loc[i], test.loc[i]))\n",
    "    \n",
    "# now we drop the cagegorised income, we only needed it to stratify\n",
    "strat_train_set = strat_train_set.drop(\"income_cat\", axis=1)\n",
    "strat_test_set = strat_test_set.drop(\"income_cat\", axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize data, first the geographical data\n",
    "housing = strat_train_set\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1\n",
    "             , s=np.log(housing[\"population\"]), c=\"median_house_value\"\n",
    "             , cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
    "            label=\"population\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the correlation matrix for all attributes, and sort for correlation with house_value\n",
    "correlation = housing.corr()\n",
    "correlation[\"median_house_value\"].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce the matrix of scatterplots, here only for three attributes\n",
    "from pandas.plotting import scatter_matrix\n",
    "attributes=[\"median_house_value\", \"median_income\", \"total_rooms\"]\n",
    "scatter_matrix(housing[attributes], figsize=(12,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that in the most important scatter-plot (income vs value) we have quirks in the data\n",
    "# i.e. horizontal lines that must have artificial origin\n",
    "housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\", alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since neither the total number of rooms / bedrooms (per district) nor the total\n",
    "# population seem meaningfull, we combine with number of households and recalculate\n",
    "# correlation to see that (obviously) rooms-per-house correlates and (surprisingly)\n",
    "# bedrooms-per-room had a strong (negative) correlation\n",
    "housing[\"rooms_per_house\"] = housing[\"total_rooms\"] / housing[\"households\"]\n",
    "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "housing[\"population_per_house\"]= housing[\"population\"] / housing[\"households\"]\n",
    "housing.corr()[\"median_house_value\"].sort_values()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a custom transformer that performs the above additions\n",
    "\n",
    "# we base our new class on two others (i.e. we \"derive\" two classes ?)\n",
    "from sklearn.base import BaseEstimator    # provides get_params() and set_params()\n",
    "from sklearn.base import TransformerMixin # provides fit_transform automatically\n",
    "\n",
    "roomsIDx, bedroomsIDx, populationIDx, householdIDx = 3,4,5,6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin) :\n",
    "    def __init__(self, add_bedrooms_per_room = True):\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room  # make this an optional addition, default=true\n",
    "        \n",
    "    def fit(self, X, y=None) : # one method that *must* be implemented in a transformer\n",
    "        return self            # but our fit doesn't need to do anything\n",
    "    \n",
    "    def transform(self, X, y=None) : # the other method that *must* be implemented\n",
    "        rooms_per_house = X[:, roomsIDx] / X[:, householdIDx]\n",
    "        population_per_house = X[:, populationIDx] / X[:, householdIDx]\n",
    "        bedrooms_per_room = X[:, bedroomsIDx] / X[:, roomsIDx]\n",
    "        newX = np.c_[X, rooms_per_house, population_per_house]\n",
    "        if (self.add_bedrooms_per_room) :\n",
    "            newX = np.c_[newX, bedrooms_per_room]\n",
    "        return newX\n",
    "    \n",
    "print(\"from: \", housing.values.shape)\n",
    "attrAdder = CombinedAttributesAdder(add_bedrooms_per_room=True)\n",
    "print (\"to: \", attrAdder.transform(housing.values).shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data, we start with our stratified split, and now we also separate\n",
    "# the predictors (\"x-data\") from the labels \"y-value\"\n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()\n",
    "\n",
    "# Most columns are complete, but for some districts the column total_bedrooms\n",
    "# is missing. We could crop the complete column (with drop) or use \"dropna\" to remove districts\n",
    "# that dont have this value or use \"fillna\" to complete those missing values with the median of others:\n",
    "# (the correct approach is to later use the exact same median of training data for missing test data)\n",
    "housing_noNA = housing.dropna(subset=[\"total_bedrooms\"])   # doesn't change housing itself\n",
    "housing.fillna(housing[\"total_bedrooms\"].median(), inplace=True) # changes housing due to inplace=true\n",
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same processing is implemented in sklearn by the Imputer class. We must remove the ocean-proximity \n",
    "# because it is non-numeric and would lead to an error, also Imputer produces a numpy-array which we \n",
    "# must transform back in a dataframe. The important point: the Imputer is a Transformer, and data-preparation\n",
    "# is typically performed in a pipeline of different transformers, each of them is instantiated, then\n",
    "# prepared with a transformer.fit() e.g. to calculate here the median and applied with transformer.transform()\n",
    "\n",
    "from sklearn.impute import SimpleImputer \n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "imputer = sklearn.impute.SimpleImputer(strategy=\"median\")\n",
    "imputer.fit(housing_num)\n",
    "(imputer.statistics_ - housing_num.median().values).sum()  # check: imputer.statistics_ has median values\n",
    "housing_fillna = pd.DataFrame(imputer.transform(housing_num), columns=housing_num.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ILLUSTRATION of one-hot encoding of categorical attribute\n",
    "# with factorize we could transform text to numeric categories\n",
    "prox_col, prox_cats = housing[\"ocean_proximity\"].factorize()\n",
    "prox_col     # array([0, 0, 1, 2, 0, 2, 0, 2, 0, 0, .....])\n",
    "prox_cats    # Index(['<1H OCEAN', 'NEAR OCEAN', 'INLAND', 'NEAR BAY', 'ISLAND'], dtype='object')\n",
    "\n",
    "\n",
    "# but ML-routines would assume category 3 and 4 to be more similar\n",
    "# than 3 and 5, which is not the case. For unrelated categories,\n",
    "# a one-hot encoding is better: each value is replaced by an \n",
    "# scipy.sparsearray of booleans with only one true-value. \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(categories='auto')         # categories=auto: if all values are either 3 or 17\n",
    "                                                   # the one-hot encoder has length 2, not 17\n",
    "prox_col = prox_col.reshape(-1,1)                  # reshapes to 2D array with one column\n",
    "prox_col_oneHot = encoder.fit_transform(prox_col)\n",
    "\n",
    "# the old method required to transform text to ints, then reshape\n",
    "# now conversion to ints no longer needed, the reshape (which requires numpy.array) still is:\n",
    "encoder = OneHotEncoder()   # \n",
    "prox_col_oneHot = encoder.fit_transform(np.array(housing[\"ocean_proximity\"]).reshape(-1,1))\n",
    "\n",
    "# toarray() produces a conventional dense array from a scipy sparse array \n",
    "# here for illustration purpose only:\n",
    "print(prox_col_oneHot.toarray())\n",
    "encoder.categories_\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will combine all our pre-processing in a pipeline: \n",
    "# - the imputer to add missing values\n",
    "# - the combined-attribute-adder to add e.g. bedrooms-per-room\n",
    "# - one-hot-encoding categorical values\n",
    "# - (not described yet) scaling all numerical values (either min-max or standardisation)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler   # we dont use this one\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer \n",
    "\n",
    "# the strategy is, that we first extract-transform numeric values only, in a second pipeline\n",
    "# extract-transform the categorical values and finally combine the two. \n",
    "\n",
    "# first we re-create our training data set (predictors only)\n",
    "housing_ = strat_train_set.drop(\"median_house_value\", axis=1)  \n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()\n",
    "\n",
    "# define a transformer that selects columns from a Dataframe and returns them as numpy.ndArray\n",
    "# ?? what are the rules for the input-type of an sklearn-transformer (Dataframe vs ndarray) ???\n",
    "class ColumnSelector(BaseEstimator, TransformerMixin) :\n",
    "    def __init__(self, column_names) :\n",
    "        self.column_names = column_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.column_names].values\n",
    "\n",
    "# NUMERIC PIPELINE\n",
    "numericColumns = list(housing_.drop(\"ocean_proximity\", axis=1))\n",
    "num_pipeline = Pipeline([\n",
    "    ('selectNumColumns', ColumnSelector(numericColumns)),\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('attAdder', CombinedAttributesAdder()),         # note we use default=true for addBrPerR\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# CATEGORICAL PIPELINE\n",
    "cat_pipeline = Pipeline([\n",
    "    ('selectCatColumns', ColumnSelector([\"ocean_proximity\"])),\n",
    "    ('catEncoder', OneHotEncoder(categories=\"auto\"))\n",
    "])\n",
    "\n",
    "# MERGE the two PIPELINES\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "pipeline = FeatureUnion(transformer_list=[\n",
    "    (\"num_pipeline\", num_pipeline),\n",
    "    (\"cat_pipeline\", cat_pipeline)\n",
    "])\n",
    "\n",
    "# and apply the full pipeline in one step\n",
    "housing_processed = pipeline.fit_transform(housing_)\n",
    "housing_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit with linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_processed, housing_labels)\n",
    "\n",
    "# check by plotting a few data points with versus their prediction\n",
    "some_data = housing.iloc[:50]\n",
    "some_labels = housing_labels.iloc[:50]\n",
    "predictions = lin_reg.predict(pipeline.transform(some_data))\n",
    "test = pd.DataFrame({\n",
    "    \"actual\": some_labels,\n",
    "    \"predicted\": predictions\n",
    "})\n",
    "test.plot(kind=\"scatter\", x=\"actual\", y=\"predicted\")\n",
    "\n",
    "# and check by calculating rmse-error on the full set\n",
    "from sklearn.metrics import mean_squared_error\n",
    "predictions = lin_reg.predict(housing_processed)\n",
    "lin_rmse = np.sqrt(mean_squared_error(housing_labels, predictions))\n",
    "print(\"RMSE: \", lin_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with a decision tree we get a error = 0 on training data\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "myTree = DecisionTreeRegressor()\n",
    "myTree.fit(housing_processed, housing_labels)\n",
    "predictions = myTree.predict(housing_processed)\n",
    "lin_rmse = np.sqrt(mean_squared_error(housing_labels, predictions))\n",
    "print(\"RMSE TREE: \", lin_rmse)\n",
    "\n",
    "# need to perform cross-validation for a more realistic measure\n",
    "# scoring must be a function \"greater is better\"\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scoresFold5 = cross_val_score(myTree, housing_processed, housing_labels, scoring = \"neg_mean_squared_error\", cv=5)\n",
    "rmseFold5 = np.sqrt(- scoresFold5)\n",
    "print (\"cross validation TREE,  mean:\", rmseFold5.mean(), \" st-dev:\",  rmseFold5.std())\n",
    "\n",
    "# so it's actually worse than the linear regression (which whould show if we'd do cross validation)\n",
    "\n",
    "# a random forest is an example of ensemble learning: pool the results\n",
    "# of a number of trees that have been trained on different subsets\n",
    "# it is much better, but still overfitting: rmse much smaller than cross-validated rmse\n",
    "# (this takes almost a minute on my brix)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "myForest = RandomForestRegressor(n_estimators = 5)\n",
    "myForest.fit(housing_processed, housing_labels)\n",
    "print(\"RMSE FOREST: \", np.sqrt(mean_squared_error(housing_labels, myForest.predict(housing_processed))))\n",
    "scoresFold5 = cross_val_score(myForest, housing_processed, housing_labels, scoring=\"neg_mean_squared_error\", cv=5)\n",
    "rmseFold5 = np.sqrt(- scoresFold5)\n",
    "print (\"cross validation FOREST,  mean:\", rmseFold5.mean(), \" st-dev:\",  rmseFold5.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINE TUNING ausgelassen:\n",
    "# suche von hyper parameter mit grid/random search\n",
    "\n",
    "# exercises auch noch ausgelassen, zB support vector machine"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
