{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "scaler = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create quadratic dataset\n",
    "def create_data_points(m) :\n",
    "    x = 6 * np.random.rand(m, 1) - 3   # 100 random numbers between -3 and +3\n",
    "    y = 0.5*x*x + x + 2                # parabola with three parameter\n",
    "    y = y + np.random.randn(m, 1)      # add gaussian noise (randn versus rand)\n",
    "    return x,y\n",
    "\n",
    "x,y = create_data_points(100)\n",
    "plt.plot(x,y,'.')\n",
    "plt.show()\n",
    "\n",
    "# The above would work with calls rand(m) instead of rand(m,1) but\n",
    "# below (for fitting) we need x and y to be 2D-arrays (with one column)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polynomial regression = linear regression on polynomial coefficients\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=True)\n",
    "x_poly = poly_features.fit_transform(x)\n",
    "x_poly.shape                            # [x, x*x], with include_bias we'd have: [1.0, x, x*x]\n",
    "lr = LinearRegression()                 # i.e. apply normal equation\n",
    "lr.fit(x_poly, y)\n",
    "print(\"lin reg found intercept:\", lr.intercept_, ' coefficients: ', lr.coef_)\n",
    "\n",
    "# more elegant two steps in a pipeline, exact same result:\n",
    "pr = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    (\"lr\", LinearRegression())\n",
    "])\n",
    "pr.fit(x,y)\n",
    "lr = pr.named_steps['lr']   # needed here only to access coefficients\n",
    "print(\"pol reg found intercept:\", lr.intercept_, ' coefficients: ', lr.coef_)\n",
    "\n",
    "# use the pipeline-created model to predict\n",
    "x_ = np.linspace(-3,3,20).reshape(20,1) # again we need a 2D array\n",
    "y_ = pr.predict(x_)\n",
    "plt.plot(x,y,'.')\n",
    "plt.plot(x_,y_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suche per batch, stochastic or minibatch and plot convergence\n",
    "eta0 = 1.0       # I need to scale by 100, 1, 10 for batch, stoch, minibatch resp. \n",
    "                 # for acceptable stability. seems related to num points, but isn't really ?\n",
    "miniSize = 10    # size of minibatch\n",
    "epoch = len(x)\n",
    "\n",
    "\n",
    "# create startpoint in lower left, plot 'S'\n",
    "# two for each method Batch, Stochastic, Minibatch: the _p will be used for to collect step\n",
    "# ts with constanet step, tsa is annealing, tm = minibatch is annealing only\n",
    "tb = tbp = ts = tsp =tsa = tsap = tm = tmp = np.random.rand(3,1) - np.array([1,1,1]).reshape(3,1)\n",
    "plt.text(tb[1], tb[2], 'S')\n",
    "\n",
    "# we need offset = bias = x0 in our polyfeatures\n",
    "x_poly = PolynomialFeatures(degree=2, include_bias=True).fit_transform(x)  # xi = [1.0, xi, xi*xi]\n",
    "\n",
    "#  # havent quite understood yet: has a major effect: can use 10 times higher eta\n",
    "#  # and convergence is no longer curved, but the resulting theta is a different one !!\n",
    "#  x_poly = scaler.fit_transform(x_poly, y)\n",
    "\n",
    "m = x_poly.shape[0]\n",
    "print(\"m:\", m)\n",
    "\n",
    "# batch gradient descent (no sklearn implementation ?)\n",
    "# NOTE: 5x more steps, each of them!!\n",
    "for step in range(1,epoch) :\n",
    "    gb = (2/m) * x_poly.T.dot(x_poly.dot(tb) - y)\n",
    "    eta = eta0 /100\n",
    "    tb = tb - eta*gb \n",
    "    tbp = np.hstack((tbp, tb))\n",
    "    \n",
    "# stochastic = one datapoint per step\n",
    "for step in range(1,epoch) :\n",
    "    ri = np.random.randint(len(x_poly))\n",
    "    gs = (2/m) * x_poly[ri:ri+1].T.dot(x_poly[ri:ri+1].dot(ts) - y[ri:ri+1])\n",
    "    eta =  eta0 \n",
    "    ts = ts - eta*gs\n",
    "    tsp = np.hstack((tsp, ts))\n",
    "\n",
    "# stochastic (annealing) = one datapoint per step, eta decreasing\n",
    "for step in range(1,epoch) :\n",
    "    ri = np.random.randint(len(x_poly))\n",
    "    gsa = (2/m) * x_poly[ri:ri+1].T.dot(x_poly[ri:ri+1].dot(tsa) - y[ri:ri+1])\n",
    "    eta = ((0.99**step) * eta0 )\n",
    "    tsa = tsa - eta*gsa\n",
    "    tsap = np.hstack((tsap, tsa))\n",
    "\n",
    "# minibatch = minibatch of datapoints per step\n",
    "for step in range(1,epoch) :\n",
    "    ri = np.random.randint(0,len(x_poly), miniSize)\n",
    "    gm = (2/m) * x_poly[ri].T.dot(x_poly[ri].dot(tm) - y[ri])   \n",
    "    eta = ((0.99**step) * eta0 )/ miniSize\n",
    "    tm = tm - eta*gm\n",
    "    tmp = np.hstack((tmp, tm))\n",
    "    \n",
    "plt.text(1.0, 0.5, '*')\n",
    "plt.text(1.0, 0.0, 'batch', color='b')\n",
    "plt.text(1.0, -0.2, 'stoch', color='r')\n",
    "plt.text(1.0, -0.4, 'annealing')\n",
    "plt.text(1.0, -0.6, 'minibatch', color='g')\n",
    "plt.plot(tbp[1], tbp[2], 'b')\n",
    "plt.plot(tsp[1], tsp[2], 'r')\n",
    "plt.plot(tsap[1], tsap[2], 'k')\n",
    "plt.plot(tmp[1], tmp[2], 'g')\n",
    "        \n",
    "plt.axis([-1,2,-1,2])\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# not perfect because we have only 100 datapoints\n",
    "# especially offset is very unprecise\n",
    "print (\"batch:\", tb)  \n",
    "print (\"target:\", np.array([2, 1, 0.5]).reshape(3,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we over-fit the data, by having polynomial with n>>2\n",
    "def check_for_n(n) :\n",
    "    \n",
    "    model = Pipeline([\n",
    "        (\"pf\", PolynomialFeatures(degree=n, include_bias=False)),\n",
    "        (\"lr\", LinearRegression())\n",
    "    ])\n",
    "\n",
    "    # to validate we need training and validation set\n",
    "    X_train, X_val, y_train, y_val = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "    # we train first with just one point, then with increasingly more, \n",
    "    # for each training we calculate training- and validation error\n",
    "    train_error, val_error = [], []  # these are python lists\n",
    "    for m in range(1,len(X_train)):\n",
    "        model.fit(X_train[:m], y_train[:m])\n",
    "        y_train_pred = model.predict(X_train[:m])\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        train_error.append(mean_squared_error(y_train_pred, y_train[:m]))\n",
    "        val_error.append(mean_squared_error(y_val_pred, y_val))\n",
    "    \n",
    "    plt.plot(np.sqrt(train_error), 'r-+')\n",
    "    plt.plot(np.sqrt(val_error), 'b')\n",
    "    plt.axis([0,len(x),0,3])\n",
    "    plt.show()\n",
    "    \n",
    "x,y = create_data_points(100)\n",
    "print(\"n=1: underfitted: error too high\")\n",
    "check_for_n(1)\n",
    "print(\"n=2\")\n",
    "check_for_n(2)\n",
    "print(\"n=10: overfitting: val-error > train-error\")\n",
    "check_for_n(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the effect of regularisations: first a comparison of ridge (normal equation) \n",
    "# versus gradient descent \n",
    "\n",
    "def poly(x):\n",
    "    return PolynomialFeatures(degree=8, include_bias=True).fit_transform(x)\n",
    "\n",
    "def scaledPoly(x):\n",
    "    return scaler.transform(poly(x))\n",
    "\n",
    "# define the points where we want to plot predictions\n",
    "x_ = np.linspace(-3,3,30).reshape(30,1) # \n",
    "scaler.fit(poly(x_))  \n",
    "\n",
    "# create and plot datapoints on a parabola with noise\n",
    "x,y = create_data_points(15)\n",
    "plt.plot(x,y,'o')\n",
    "\n",
    "# linear Regression (i.e. solving normal equation, not iterative)\n",
    "ridge = Ridge(alpha=1e-3, solver=\"cholesky\")\n",
    "ridge.fit(scaledPoly(x),y)\n",
    "y_ = ridge.predict(scaledPoly(x_))\n",
    "plt.plot(x_,y_,'r-', linewidth=0.4, label=\"ridge alpha=1e-3\")\n",
    "\n",
    "ridge = Ridge(alpha=1, solver=\"cholesky\")\n",
    "ridge.fit(scaledPoly(x),y)\n",
    "y_ = ridge.predict(scaledPoly(x_))\n",
    "plt.plot(x_,y_,'r-', linewidth=0.8, label=\"ridge alpha=1\")\n",
    "\n",
    "# similar results withgradient descent SGDRegressor\n",
    "# y must be a 1D list now (unstable without scaling!)\n",
    "sgd_reg = SGDRegressor(penalty=\"l2\", alpha=1e-3, tol=1E-5, max_iter=10000)\n",
    "sgd_reg.fit(scaler.transform(scaledPoly(x)), y.ravel())\n",
    "y_ = sgd_reg.predict(scaler.transform(scaledPoly(x_)))\n",
    "plt.plot(x_,y_,'r.', linewidth=1, label=\"sgd ridge alpha=1e-3\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.axis([-3,3,0,10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now compare ridge to lasso to elastic net\n",
    "# regularisation with l2-norm, l1-norm or a mixture of both\n",
    "# I dont see major differences\n",
    "x,y = create_data_points(10)\n",
    "\n",
    "# ridge using normal equation\n",
    "ridge = Ridge(alpha=1e-3, solver=\"cholesky\")\n",
    "ridge.fit(scaledPoly(x),y)\n",
    "y_ = ridge.predict(scaledPoly(x_))\n",
    "plt.plot(x_,y_,'b-', linewidth=0.5, label=\"ridge alpha=1e-3\")\n",
    "\n",
    "# lasso is by definition gradient descent ?\n",
    "lasso = Lasso(alpha=1e-3, tol=1e-3, max_iter=10000)\n",
    "lasso.fit(scaledPoly(x),y)\n",
    "y_ = lasso.predict(scaledPoly(x_))\n",
    "plt.plot(x_,y_,'g-', linewidth=0.5,  label=\"lasso\")\n",
    "\n",
    "# should be exact same result but isnt ?\n",
    "lasso = SGDRegressor(penalty=\"l1\", alpha=1e-3, tol=1e-3, max_iter=1000)\n",
    "lasso.fit(scaledPoly(x),y.ravel())\n",
    "y_ = lasso.predict(scaledPoly(x_))\n",
    "plt.plot(x_,y_,'gx', linewidth=0.5,  label=\"sgd-lasso\")\n",
    "\n",
    "# elastic net\n",
    "net = ElasticNet(alpha=1e-3, tol=1e-3, max_iter=10000, l1_ratio=0.5)\n",
    "net.fit(scaledPoly(x),y)\n",
    "y_ = net.predict(scaledPoly(x_))\n",
    "plt.plot(x_,y_,'r-', linewidth=0.5, label=\"elastic net\")\n",
    "\n",
    "plt.plot(x,y,'bo')\n",
    "plt.legend()\n",
    "plt.axis([-3,3,0,10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# und jetzt noch den Iris Datensatz klassifizieren\n",
    "# jeweils 4 Messergebnisse an 150 Blueten\n",
    "iris = datasets.load_iris()\n",
    "df = pandas.DataFrame(iris['data'], columns=iris['feature_names'])\n",
    "print(\"features (1st rows):\", df[:5])\n",
    "print(\"target ist 1D numpy.array laenge 150\")\n",
    "\n",
    "# learn then classify one label one one feature\n",
    "petalWidth = iris['data'][:,3].reshape(-1,1)  # need as 2D array, each feature a one-element row\n",
    "isC = iris['target'] == 2\n",
    "logreg = LogisticRegression(solver='lbfgs')\n",
    "logreg.fit(petalWidth, isC)  \n",
    "predC = logreg.predict(petalWidth)\n",
    "print(\"one feature one label, incorrect: \", (isC != predC).sum())\n",
    "\n",
    "# now with all features, nothing really changes, exept better result\n",
    "isC = iris['target'] == 2\n",
    "x = iris['data']\n",
    "logreg = LogisticRegression(solver='lbfgs')\n",
    "logreg.fit(x, isC)  \n",
    "predC = logreg.predict(x)\n",
    "print(\"all features one label, incorrect: \", (isC != predC).sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiclass prediction\n",
    "\n",
    "# In chapter 2 and 3 we just defined target as a one-hot encoded array: \n",
    "# this worke for Linear Regression, Random Foreset and stochastic gradient descent\n",
    "# but it isn't accepted by LogisticRegression (probably because below method better)\n",
    "# following gives error\n",
    "# encoder = OneHotEncoder(categories='auto')\n",
    "# y_as_onehot = encoder.fit_transform(iris['target'].reshape(-1,1))\n",
    "# logreg.fit(x, y_as_onehot)\n",
    "\n",
    "# Logistic Regression supports \"multiclass\" (see book): \n",
    "# decision based on score, (actuall softmax function of score)\n",
    "# trained by minimizing cross entropy, the gradient of which is (p-y)*x, \n",
    "\n",
    "soft_reg = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", C=10)\n",
    "x = iris[\"data\"][:,(2,3)]  # two features = two columns\n",
    "y = iris[\"target\"]\n",
    "soft_reg.fit(x, iris[\"target\"])\n",
    "\n",
    "# plot the result\n",
    "colormapBG = ListedColormap(['#FFDDDD', '#DDFFDD', '#DDDDFF'])\n",
    "colormapFG = ListedColormap(['#AA0000', '#00AA00', '#0000AA'])\n",
    "plt.xlim(0,8)\n",
    "plt.ylim(0,3)\n",
    "xx, yy = np.meshgrid(np.arange(0,8,0.1), np.arange(0,3,0.1))\n",
    "Z = soft_reg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.pcolormesh(xx,yy,Z, cmap=colormapBG)\n",
    "plt.scatter(x[:,0], x[:,1], c=y, cmap=colormapFG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
