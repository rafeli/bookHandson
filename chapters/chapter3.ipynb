{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# return_X_y=true gibt nur das Tupel von np.arrays\n",
    "# mnist = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "\n",
    "# default ist ein object mit 'data', 'target', ... attributes\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = mnist[\"data\"], mnist[\"target\"]\n",
    "print(X.shape)\n",
    "sampleDigit = X[36000]\n",
    "plt.imshow(sampleDigit.reshape(28,28), cmap = matplotlib.cm.binary, interpolation = \"nearest\")\n",
    "# plt.axis(\"off\")\n",
    "plt.show\n",
    "y[36000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meine Spielerei: 10x10 images zusammenstellen\n",
    "\n",
    "randomIndices = np.random.permutation(100)\n",
    "digitTable = X[randomIndices]\n",
    "digitTable.shape\n",
    "\n",
    "def digit(i) :\n",
    "    myDigit = X[randomIndices[i]]\n",
    "    return myDigit.reshape(28,28)\n",
    "\n",
    "# print(digit(27))\n",
    "\n",
    "for i in range(0, 9) :\n",
    "    myRow = digit(10*i)\n",
    "    for j in range(1, 9) : \n",
    "        myRow = np.hstack((myRow, digit(10*i+j)))\n",
    "    if i == 0:\n",
    "        digitTable = myRow\n",
    "    else :\n",
    "        digitTable = np.vstack((digitTable, myRow))\n",
    "            \n",
    "\n",
    "plt.imshow(digitTable, cmap = matplotlib.cm.binary, interpolation = \"nearest\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist is already split in 60,000 training and 10,000 test data\n",
    "# we shuffle the training data to have a valid basis for crossvalidation\n",
    "shuffle = np.random.permutation(60000)\n",
    "x_train, x_test, y_train, y_test = X[:60000][shuffle], X[60000:], y[:60000][shuffle], y[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a binary classifier (yes/no) on predicting it's a five or not\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd = SGDClassifier(random_state=42, tol=1e-3, max_iter=1000)\n",
    "sgd.fit(x_train, y_train == '5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement cross-validation ourselves\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "y_train_5 = (y_train == '5')\n",
    "skfolds = StratifiedKFold(n_splits=3, random_state=42)\n",
    "for train_index, test_index in skfolds.split(x_train, y_train_5):\n",
    "    fitter = clone(sgd)\n",
    "    fitter.fit(x_train[train_index], y_train_5[train_index])\n",
    "    y_pred = fitter.predict(x_train[test_index])\n",
    "    print(sum(y_pred == y_train_5[test_index]) / len(y_pred))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the identical cross-validation, now with sklearn\n",
    "\n",
    "# accuracy not the right measure because only 10% of samples is a '5'\n",
    "# e.g. the trivial predictor always outputting 'false' has accuracy 90%\n",
    "# confuction matrix gives TP, FP, FN, TN and is calculated \n",
    "\n",
    "#  # this would give the exact same result as our implementation of cross-validation\n",
    "#  from sklearn.model_selection import cross_val_score\n",
    "#  cross_val_score(sgd, x_train, (y_train == '5'), cv=3, scoring=\"accuracy\")\n",
    "\n",
    "# use cross validation to calculate a predcition instead of a score, \n",
    "# from which we calculate a confusion matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_train_5 = (y_train == '5')\n",
    "y_train_pred = cross_val_predict(sgd, x_train, (y_train_5), cv=6, n_jobs=-1)\n",
    "m = confusion_matrix(y_train_5, y_train_pred)\n",
    "\n",
    "\n",
    "# confusion_matrix(test, predicted) gives:\n",
    "# \n",
    "#  TN  FP    --  first row: all negativ cases\n",
    "#  FN  TP\n",
    "#       |\n",
    "#      second column: predicted to be positive\n",
    "#\n",
    "print(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m)\n",
    "TN = m[0,0]\n",
    "FP = m[0,1]\n",
    "FN = m[1,0]\n",
    "TP = m[1,1]\n",
    "print(\"accuracy = (TP + TN) / (TP + FP + TN + FN) = \", (TP+TN)/y_train.size)\n",
    "\n",
    "print(\"precision = TP / (TP + FP) = \", TP/(TP+FP))\n",
    "print(\"TPR = recall = TP / (TP + FN)\", TP / (TP+FN))\n",
    "print(\"F1 = harm. mean(precision,TPR) = 2*TP/(2TP+FN+FP)\", 2*TP/(2*TP + FN+ FP))\n",
    "\n",
    "# same thing from sklearn.metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "print(\"precision: \", precision_score(y_train_5, y_train_pred))\n",
    "print(\"recall: \", recall_score(y_train_5, y_train_pred))\n",
    "print(\"f1: \", f1_score(y_train_5, y_train_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the yes/no prediction of SGD is based on the score being higher than\n",
    "# the threshold, which is set at zero. We can improve precision at the cost of\n",
    "# recall by setting a higher treshold\n",
    "# sklearn has no implementation to calculat confusion-matrix for \n",
    "# for different threshold, so we implement ourselves, using decision_function, \n",
    "# to make the decision ourselves\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "def getConfusion(threshold) :\n",
    "\n",
    "    TP = TN = FP = FN = 0\n",
    "    y_train_5 = (y_train == '5')\n",
    "    skfolds = StratifiedKFold(n_splits=3, random_state=42)\n",
    "    for train_index, test_index in skfolds.split(x_train, y_train_5):\n",
    "        fitter = clone(sgd)\n",
    "        fitter.fit(x_train[train_index], y_train_5[train_index])\n",
    "        y_score = fitter.decision_function(x_train[test_index])\n",
    "        y_pred = (y_score > threshold)\n",
    "        TP = TP + sum(y_pred & (y_pred == y_train_5[test_index]))\n",
    "        FP = FP + sum(y_pred & (y_pred != y_train_5[test_index]))\n",
    "        FN = FN + sum(y_train_5[test_index] & (y_pred != y_train_5[test_index]))\n",
    "\n",
    "    TN = TN + (y_train_5.size - TP - FP - FN)\n",
    "    print(\"\\n threshold:\", threshold, \"precision:\", TP / (TP+FP))\n",
    "    print(\"recall:\", TP / (TP+FN))\n",
    "    return np.array([[TN, FP], [FN, TP]])\n",
    "\n",
    "m = getConfusion(0)\n",
    "print(m)\n",
    "print(getConfusion(1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing this threshold is a trade-off between precision and recall\n",
    "# which is illustrated by precision_recall_curve and is implemented in sklearn:\n",
    "\n",
    "# predict the full set of scores\n",
    "y_train_5 = (y_train =='5')\n",
    "y_scores = cross_val_predict(sgd, x_train, y_train_5, cv=3, method=\"decision_function\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the curve\n",
    "# recall is smooth and monotonously decreasing,\n",
    "# precision increases with threshold but not necessarily\n",
    "# monotonously and therefore less smooth\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\n",
    "plt.plot(thresholds, precisions[:-1], \"b--\", label=\"precision\")\n",
    "plt.plot(thresholds, recalls[:-1], \"g-\", label=\"recall\")\n",
    "plt.xlabel(\"threshold\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the (other) classical curve is ROC, plotting \n",
    "# TPR=recall versus FPR = 1 - sensitivity\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\n",
    "plt.plot(fpr, tpr, linewidth=2)\n",
    "plt.plot([0,1], [0,1], 'k--')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.show()\n",
    "\n",
    "# the area under the curve is higher for a better predictor\n",
    "# AUC=0.5 for a random predictor, AUC=1.0 for a perfect predictor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(\"auc:\", roc_auc_score(y_train_5, y_scores))\n",
    "\n",
    "## TIP\n",
    "# use precision vs recall if FP are more important than FN\n",
    "# otherwise use ROC curve / AUC rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now a second classifier: a random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(random_state=42, n_estimators=10)\n",
    "y_predict = cross_val_predict(rfc, x_train, y_train_5,\n",
    "                               cv=3, method=\"predict_proba\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the above y_predict calculated as predict_proba are not\n",
    "# a 60000x1 array either of true/false values or scores,\n",
    "# but rather 60000x2 array with probabilities of\n",
    "# being 5 or not being 5. Select the 2nd column as a \"score\"\n",
    "# to produce ROCplt.plot(fpr, tpr, linewidth=2)\n",
    "y_score = y_predict[:,1]\n",
    "fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_train_5, y_score)\n",
    "\n",
    "plt.plot([0,1], [0,1], 'k--')\n",
    "plt.plot(fpr_rf, tpr_rf, \"b:\")\n",
    "plt.plot(fpr,tpr, \"b-\")\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.show()\n",
    "\n",
    "print(\"auc random forest:\", roc_auc_score(y_train_5, y_score))\n",
    "\n",
    "# at page95, TODO: multiclass classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now move to multiclass classification, just replace the boolean by a one-hot result in y_train\n",
    "# this takes longer on ryzen: \n",
    "sgd.fit(x_train, y_train)\n",
    "rfc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if multiclass is requested for a single class methode (e.g. SGD)\n",
    "# the strategy is to combine these single-class classifiers, either\n",
    "# one-versus-all or (n classifiers) or one-versus-one (n*n classifiers)\n",
    "\n",
    "# default (for SGDClassifier) is a one-versus-all calculation:\n",
    "# with the above sklearn trained 10 single-class-classifiers,\n",
    "# the (10) single scores are displayed with decision-function\n",
    "# and the winning class with predict, e.g. for an image of a 6\n",
    "somedigit = x_train[36002]\n",
    "print (\"sgd predicted class of some digit is: \", sgd.predict([somedigit]))\n",
    "print(\"sgd scores are:\", sgd.decision_function([somedigit]))\n",
    "\n",
    "# you could enforce ovo as follows, too slow here ...\n",
    "# would be faster in SVM because each classifier is trained with less\n",
    "# trainingsdata (containing either one) and SVM scales (very) badly with data size\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "sgd_ovo = OneVsOneClassifier(SGDClassifier(random_state=42))\n",
    "# sgd_ovoe.fit(...)\n",
    "\n",
    "\n",
    "# a random-forest does multiclass directly and has no decision_function\n",
    "# and its probability function is often 0/1\n",
    "print (\"the class of digit 2045 is: \", rfc.predict([somedigit]))\n",
    "print(\"probability:\", rfc.predict_proba([somedigit]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the accuracy is meaningfull now and high enough:\n",
    "from sklearn.model_selection import cross_val_score\n",
    "print(\"accuracy sgd:\", cross_val_score(sgd, x_train, y_train, cv=3, scoring=\"accuracy\", n_jobs=-1))\n",
    "print(\"accuracy rfc:\", cross_val_score(rfc, x_train, y_train, cv=3, scoring=\"accuracy\", n_jobs=-1))\n",
    "\n",
    "\n",
    "# the book shows lower valued that are improved by proper scaling:\n",
    "# it seems we use a different dataset that already has been scaled !!)\n",
    "# but the following code does work, it just takes long again\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train.astype(np.float64))\n",
    "# print(\"accuracy sgd scaled:\", cross_val_score(sgd, x_train_scaled, y_train, cv=3, scoring=\"accuracy\", n_jobs=-1))\n",
    "# print(\"accuracy rfc scaled:\", cross_val_score(rfc, x_train_scaled, y_train, cv=3, scoring=\"accuracy\", n_jobs=-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 10x10 confusion matrix gives more detailed info what digits are mixed up\n",
    "y_train_pred = cross_val_predict(sgd, x_train, y_train, cv=3)\n",
    "confusion = confusion_matrix(y_train, y_train_pred)\n",
    "\n",
    "# scale each confusion by number of corresponding data points\n",
    "confusion = confusion / confusion.sum(axis=1, keepdims=True)\n",
    "\n",
    "# set diagonal to zero (would mask all other values as much higher)\n",
    "np.fill_diagonal(confusion,  0)\n",
    "\n",
    "# and plot it to show that most frequent error is qualifying 8 as 5\n",
    "# also that 5 and 3 can be mixed up\n",
    "# classification as 8 (column) is often an error\n",
    "# and also that 9 (row) is hard to qualify\n",
    "plt.matshow(confusion, cmap=plt.cm.gray)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilabel classification produces y that is an array with >1 cells\n",
    "# as a first example we consider classes [large, odd] = [(y>6), (y%2==1)]\n",
    "\n",
    "# wir hatten y_train noch als string, muessen daraus int machen\n",
    "y_train = y_train.astype(int)\n",
    "y_multi = np.c_[(y_train >= 6), (y_train % 2 == 1)]\n",
    "\n",
    "# not all classifiers support multilabel, kNeighbors does\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(x_train, y_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and test for some random digits\n",
    "x_train.shape[0]\n",
    "\n",
    "randomIndices = np.random.permutation(x_train.shape[0])[0:10]\n",
    "for i in randomIndices :\n",
    "  print(i, sgd.predict([x_train[i]]), ' large:', knn.predict([x_train[i]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multioutput classification is by definition multioutput-multiclass \n",
    "# and accepts multiple values per label, i.e. produces a 2D-matrix for each data point\n",
    "\n",
    "# we use it here to clear noise from images (which we add first, the original image is our target)\n",
    "# produce noisy images:\n",
    "x_train_noisy = x_train + np.random.randint(0, 100, (x_train.shape[0], x_train.shape[1]) )\n",
    "x_test_noisy = x_test + np.random.randint(0, 100, (x_test.shape[0], x_test.shape[1]) )\n",
    "\n",
    "# train to find clean from noisy\n",
    "knn.fit(x_train_noisy, x_train)  \n",
    "\n",
    "# and apply to the random images \n",
    "# TDOO: I still donot seem to understand plots and subplots ...\n",
    "rows = np.zeros([0, 56])\n",
    "for i in randomIndices :\n",
    "    compare = np.hstack((x_train_noisy[i].reshape(28,28), x_train[i].reshape(28,28)))\n",
    "    rows = np.vstack((rows, compare))\n",
    "        \n",
    "plt.imshow(rows, cmap = matplotlib.cm.binary, interpolation = \"nearest\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
